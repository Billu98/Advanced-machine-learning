{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ijGzTHJJUCPY"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDsTUvKjwHBW"
      },
      "source": [
        "# Multimodal Retrieval Augmented Generation (RAG) with Gemini, Vertex AI Vector Search, and LangChain\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fuse-cases%2Fretrieval-augmented-generation%2Fmultimodal_rag_langchain.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/multimodal_rag_langchain.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/use-cases/retrieval-augmented-generation/multimodal_rag_langchain.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "    <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/retrieval-augmented-generation/multimodal_rag_langchain.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37454218170d"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Holt Skinner](https://github.com/holtskinner) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK1Q5ZYdVL4Y"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Retrieval augmented generation (RAG) has become a popular paradigm for enabling LLMs to access external data and also as a mechanism for grounding to mitigate against hallucinations.\n",
        "\n",
        "In this notebook, you will learn how to perform multimodal RAG where you will perform Q&A over a financial document filled with both text and images.\n",
        "\n",
        "### Gemini\n",
        "\n",
        "Gemini is a family of generative AI models developed by Google DeepMind that is designed for multimodal use cases. The Gemini API gives you access to the Gemini 1.0 Pro Vision and Gemini 1.0 Pro models.\n",
        "\n",
        "### Comparing text-based and multimodal RAG\n",
        "\n",
        "Multimodal RAG offers several advantages over text-based RAG:\n",
        "\n",
        "1. **Enhanced knowledge access:** Multimodal RAG can access and process both textual and visual information, providing a richer and more comprehensive knowledge base for the LLM.\n",
        "2. **Improved reasoning capabilities:** By incorporating visual cues, multimodal RAG can make better informed inferences across different types of data modalities.\n",
        "\n",
        "This notebook shows you how to use RAG with Vertex AI Gemini API, and [multimodal embeddings](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/multimodal-embeddings), to build a document search engine.\n",
        "\n",
        "Through hands-on examples, you will discover how to construct a multimedia-rich metadata repository of your document sources, enabling search, comparison, and reasoning across diverse information streams."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQT500QqVPIb"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "This notebook provides a guide to building a document search engine using multimodal retrieval augmented generation (RAG), step by step:\n",
        "\n",
        "1. Extract and store metadata of documents containing both text and images, and generate embeddings the documents\n",
        "2. Search the metadata with text queries to find similar text or images\n",
        "3. Search the metadata with image queries to find similar images\n",
        "4. Using a text query as input, search for contextual answers using both text and images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnpYxfesh2rI"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "- Vertex AI\n",
        "\n",
        "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXJpXzKrh2rJ"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5afkyDMSBW5"
      },
      "source": [
        "### Install Vertex AI SDK for Python and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kc4WxYmLSBW5"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q google-cloud-aiplatform langchain-core langchain-google-vertexai langchain-text-splitters langchain-community \"unstructured[all-docs]\" pypdf pydantic lxml pillow matplotlib opencv-python tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-cloud-storage\n",
        "!pip install --upgrade langchain langchain-core langchain-google-vertexai unstructured\n",
        "!apt-get install -y poppler-utils\n",
        "!apt install -y tesseract-ocr\n",
        "!pip install pytesseract\n",
        "!pip install --upgrade nltk\n",
        "!pip install chromadb\n",
        "!pip install --upgrade pydantic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vjeP6_VzgwXP",
        "outputId": "ab87f715-ae23-4923-82be-b24059387c66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (1.44.0)\n",
            "Collecting google-cloud-storage\n",
            "  Using cached google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.27.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (1.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n",
            "Using cached google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
            "Installing collected packages: google-cloud-storage\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 1.44.0\n",
            "    Uninstalling google-cloud-storage-1.44.0:\n",
            "      Successfully uninstalled google-cloud-storage-1.44.0\n",
            "Successfully installed google-cloud-storage-2.18.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "beb730c5d41843fa93977aaabf623de1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.300)\n",
            "Collecting langchain\n",
            "  Using cached langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.3.15)\n",
            "Requirement already satisfied: langchain-google-vertexai in /usr/local/lib/python3.10/dist-packages (2.0.7)\n",
            "Requirement already satisfied: unstructured in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.139-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.70.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (1.71.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (2.18.2)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (0.27.2)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-google-vertexai) (0.4.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n",
            "Requirement already satisfied: filetype in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.2.0)\n",
            "Requirement already satisfied: python-magic in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.4.27)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.3.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.9.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.14.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.7)\n",
            "Requirement already satisfied: python-iso639 in /usr/local/lib/python3.10/dist-packages (from unstructured) (2024.10.22)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.0.9)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.10.1)\n",
            "Requirement already satisfied: backoff in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.2.1)\n",
            "Requirement already satisfied: unstructured-client in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.26.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.66.6)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.9.5)\n",
            "Requirement already satisfied: python-oxmsg in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.0.1)\n",
            "Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (3.20.3)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.13.0)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (2.0.6)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (0.16)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain-google-vertexai) (1.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.6)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->unstructured) (0.9.0)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib->unstructured) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2024.9.11)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.10/dist-packages (from python-oxmsg->unstructured) (0.47)\n",
            "Requirement already satisfied: cryptography>=3.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (43.0.3)\n",
            "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (0.2.0)\n",
            "Requirement already satisfied: jsonpath-python<2.0.0,>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.0.6)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (1.6.0)\n",
            "Requirement already satisfied: pypdf>=4.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (5.1.0)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->unstructured) (2.8.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.1->unstructured-client->unstructured) (1.17.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.65.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (4.9)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (0.13.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.0.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->langchain-google-vertexai) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.1->unstructured-client->unstructured) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.70.0->langchain-google-vertexai) (0.6.1)\n",
            "Using cached langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
            "Downloading langsmith-0.1.139-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langsmith, langchain\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.0.92\n",
            "    Uninstalling langsmith-0.0.92:\n",
            "      Successfully uninstalled langsmith-0.0.92\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.300\n",
            "    Uninstalling langchain-0.0.300:\n",
            "      Successfully uninstalled langchain-0.0.300\n",
            "Successfully installed langchain-0.3.7 langsmith-0.1.139\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (11.0.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (0.5.17)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.9.2)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.115.4)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.20.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.27.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.19.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.6)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.4.5)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.64.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.2.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.12.5)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (31.0.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.0.1)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.10)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.27.2)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.3)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.0.2)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (0.41.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
            "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.27.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.48b0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (75.1.0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.23.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.24.7)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.9.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Xep4W9lq-Z"
      },
      "source": [
        "### Restart current runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XRvKdaPDTznN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7abd696-fd87-41f5-cbf0-df7a1ab3ca8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbmM4z7FOBpM"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Please wait until it is finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtsU9Bw9h2rL"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)\n",
        "\n",
        "If you are running this notebook on Google Colab, run the following cell to authenticate your environment. This step is not required if you are using [Vertex AI Workbench](https://cloud.google.com/vertex-ai-workbench)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GpYEyLsOh2rL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1vKZZoEh2rL"
      },
      "source": [
        "### Define Google Cloud project information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gJqZ76rJh2rM"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"gen-lang-client-0784670847\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "# For Vector Search Staging\n",
        "GCS_BUCKET = \"multimodalrag_lucid\"  # @param {type:\"string\"}\n",
        "GCS_BUCKET_URI = f\"gs://{GCS_BUCKET}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57262621bd1c"
      },
      "source": [
        "### Initialize the Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D48gUW5-h2rM"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=GCS_BUCKET_URI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuQwwRiniVFG"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rtMowvm-yQ97"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import os\n",
        "import re\n",
        "import uuid\n",
        "\n",
        "from IPython.display import Image, Markdown, display\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_google_vertexai import (\n",
        "    ChatVertexAI,\n",
        "    VectorSearchVectorStore,\n",
        "    VertexAI,\n",
        "    VertexAIEmbeddings,\n",
        ")\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "\n",
        "# from langchain_community.vectorstores import Chroma  # Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bf3ee5d1686"
      },
      "source": [
        "### Define model information\n",
        "\n",
        "- [Vertex AI - Model Information](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eb39bdada39d"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"gemini-1.5-flash\"\n",
        "GEMINI_OUTPUT_TOKEN_LIMIT = 8192\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"text-embedding-004\"\n",
        "EMBEDDING_TOKEN_LIMIT = 2048\n",
        "\n",
        "TOKEN_LIMIT = min(GEMINI_OUTPUT_TOKEN_LIMIT, EMBEDDING_TOKEN_LIMIT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c919bd5a462"
      },
      "source": [
        "## Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7bKCQMFT7JT"
      },
      "source": [
        "#### Get documents and images from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KwbL89zcY39N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a81253-fe6a-4263-e5f1-82346196e040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: gsutil rsync uses hashes when modification time is not available at\n",
            "both the source and destination. Your crcmod installation isn't using the\n",
            "module's C extension, so checksumming will run very slowly. If this is your\n",
            "first rsync since updating gsutil, this rsync can take significantly longer than\n",
            "usual. For help installing the extension, please see \"gsutil help crcmod\".\n",
            "\n",
            "Building synchronization state...\n",
            "Starting synchronization...\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "# Download documents and images used in this notebook\n",
        "!gsutil -m rsync -r gs://multimodalrag_lucid/lucid_data .\n",
        "print(\"Download completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps1G-cCfpibN"
      },
      "source": [
        "## Partition PDF tables, text, and images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqLsy3iZ5t-R"
      },
      "source": [
        "### The data\n",
        "\n",
        "The source data that you will use in this notebook is a modified version of [Google-10K](https://abc.xyz/assets/investor/static/pdf/20220202_alphabet_10K.pdf) which provides a comprehensive overview of the company's financial performance, business operations, management, and risk factors. As the original document is rather large, you will be using [a modified version with only 14 pages](https://storage.googleapis.com/github-repo/rag/multimodal_rag_langchain/google-10k-sample-14pages.pdf) instead. Although it's truncated, the sample document still contains text along with images such as tables, charts, and graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3a87cb1a097b"
      },
      "outputs": [],
      "source": [
        "pdf_folder_path = \"/content/data/\" if \"google.colab\" in sys.modules else \"data/\"\n",
        "pdf_file_name = [f for f in os.listdir(pdf_folder_path) if f.endswith('.pdf')]\n",
        "# pdf_file_name = \"About the Handbook _ The GitLab Handbook.pdf\"                   : previously reading 1 document\n",
        "\n",
        "# Extract images, tables, and chunk text from a PDF file.\n",
        "raw_pdf_elements = partition_pdf(\n",
        "    filename=pdf_file_name,\n",
        "    extract_images_in_pdf=False,\n",
        "    infer_table_structure=True,\n",
        "    chunking_strategy=\"by_title\",\n",
        "    max_characters=4000,\n",
        "    new_after_n_chars=3800,\n",
        "    combine_text_under_n_chars=2000,\n",
        "    image_output_dir_path=pdf_folder_path,\n",
        ")\n",
        "\n",
        "# Categorize extracted elements from a PDF into tables and texts.\n",
        "tables = []\n",
        "texts = []\n",
        "for element in raw_pdf_elements:\n",
        "    if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
        "        tables.append(str(element))\n",
        "    elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
        "        texts.append(str(element))\n",
        "\n",
        "# Optional: Enforce a specific token size for texts\n",
        "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=10000, chunk_overlap=0\n",
        ")\n",
        "joined_texts = \" \".join(texts)\n",
        "texts_4k_token = text_splitter.split_text(joined_texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "244963a30836"
      },
      "outputs": [],
      "source": [
        "# Generate summaries of text elements\n",
        "\n",
        "\n",
        "def generate_text_summaries(\n",
        "    texts: list[str], tables: list[str], summarize_texts: bool = False\n",
        ") -> tuple[list, list]:\n",
        "    \"\"\"\n",
        "    Summarize text elements\n",
        "    texts: List of str\n",
        "    tables: List of str\n",
        "    summarize_texts: Bool to summarize texts\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt\n",
        "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
        "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
        "    prompt = PromptTemplate.from_template(prompt_text)\n",
        "    empty_response = RunnableLambda(\n",
        "        lambda x: AIMessage(content=\"Error processing document\")\n",
        "    )\n",
        "    # Text summary chain\n",
        "    model = VertexAI(\n",
        "        temperature=0, model_name=MODEL_NAME, max_output_tokens=TOKEN_LIMIT\n",
        "    ).with_fallbacks([empty_response])\n",
        "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
        "\n",
        "    # Initialize empty summaries\n",
        "    text_summaries = []\n",
        "    table_summaries = []\n",
        "\n",
        "    # Apply to text if texts are provided and summarization is requested\n",
        "    if texts:\n",
        "        if summarize_texts:\n",
        "            text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 1})\n",
        "        else:\n",
        "            text_summaries = texts\n",
        "\n",
        "    # Apply to tables if tables are provided\n",
        "    if tables:\n",
        "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 1})\n",
        "\n",
        "    return text_summaries, table_summaries\n",
        "\n",
        "\n",
        "# Get text, table summaries\n",
        "text_summaries, table_summaries = generate_text_summaries(\n",
        "    texts_4k_token, tables, summarize_texts=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "379ae4ffbf83"
      },
      "outputs": [],
      "source": [
        "def encode_image(image_path: str) -> str:\n",
        "    \"\"\"Getting the base64 string\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "def image_summarize(model: ChatVertexAI, base64_image: str, prompt: str) -> str:\n",
        "    \"\"\"Make image summary\"\"\"\n",
        "    msg = model.invoke(\n",
        "        [\n",
        "            HumanMessage(\n",
        "                content=[\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"},\n",
        "                    },\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return msg.content\n",
        "\n",
        "\n",
        "def generate_img_summaries(path: str) -> tuple[list[str], list[str]]:\n",
        "    \"\"\"\n",
        "    Generate summaries and base64 encoded strings for images\n",
        "    path: Path to list of .jpg files extracted by Unstructured\n",
        "    \"\"\"\n",
        "\n",
        "    # Store base64 encoded images\n",
        "    img_base64_list = []\n",
        "\n",
        "    # Store image summaries\n",
        "    image_summaries = []\n",
        "\n",
        "    # Prompt\n",
        "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
        "    These summaries will be embedded and used to retrieve the raw image. \\\n",
        "    Give a concise summary of the image that is well optimized for retrieval.\n",
        "    If it's a table, extract all elements of the table.\n",
        "    If it's a graph, explain the findings in the graph.\n",
        "    Do not include any numbers that are not mentioned in the image.\n",
        "    \"\"\"\n",
        "\n",
        "    model = ChatVertexAI(model_name=MODEL_NAME, max_output_tokens=TOKEN_LIMIT)\n",
        "\n",
        "    # Apply to images\n",
        "    for img_file in sorted(os.listdir(path)):\n",
        "        if img_file.endswith(\".png\"):\n",
        "            base64_image = encode_image(os.path.join(path, img_file))\n",
        "            img_base64_list.append(base64_image)\n",
        "            image_summaries.append(image_summarize(model, base64_image, prompt))\n",
        "\n",
        "    return img_base64_list, image_summaries\n",
        "\n",
        "\n",
        "# Image summaries\n",
        "img_base64_list, image_summaries = generate_img_summaries(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b641a76265d0"
      },
      "source": [
        "## Create & Deploy Vertex AI Vector Search Index & Endpoint\n",
        "\n",
        "Skip this step if you already have Vector Search set up.\n",
        "\n",
        "- https://console.cloud.google.com/vertex-ai/matching-engine/indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c15693534ed1"
      },
      "source": [
        "- Create [`MatchingEngineIndex`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndex)\n",
        "  - https://cloud.google.com/vertex-ai/docs/vector-search/create-manage-index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dad379accb68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "984b2301-fe4d-4f2a-eb98-e08638c91a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:Creating MatchingEngineIndex\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:Create MatchingEngineIndex backing LRO: projects/61515807285/locations/us-central1/indexes/8034399744996409344/operations/3415520343542988800\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:MatchingEngineIndex created. Resource name: projects/61515807285/locations/us-central1/indexes/8034399744996409344\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:To use this MatchingEngineIndex in another session:\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:index = aiplatform.MatchingEngineIndex('projects/61515807285/locations/us-central1/indexes/8034399744996409344')\n"
          ]
        }
      ],
      "source": [
        "# https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings\n",
        "DIMENSIONS = 768  # Dimensions output from textembedding-gecko\n",
        "\n",
        "index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "    display_name=\"mm_rag_langchain_index\",\n",
        "    dimensions=DIMENSIONS,\n",
        "    approximate_neighbors_count=150,\n",
        "    leaf_node_embedding_count=500,\n",
        "    leaf_nodes_to_search_percent=7,\n",
        "    description=\"Multimodal RAG LangChain Index\",\n",
        "    index_update_method=\"STREAM_UPDATE\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "add71035aaa1"
      },
      "source": [
        "- Create [`MatchingEngineIndexEndpoint`](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.MatchingEngineIndexEndpoint)\n",
        "  - https://cloud.google.com/vertex-ai/docs/vector-search/deploy-index-public"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "140c0142b90f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c25cd4d-957b-4f53-c9cd-4cb341e74a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing index endpoint: projects/61515807285/locations/us-central1/indexEndpoints/7140857431428431872\n"
          ]
        }
      ],
      "source": [
        "DEPLOYED_INDEX_ID = \"mm_rag_langchain_index_endpoint\"\n",
        "\n",
        "# index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "#     display_name=DEPLOYED_INDEX_ID,\n",
        "#     description=\"Multimodal RAG LangChain Index Endpoint\",\n",
        "#     public_endpoint_enabled=True,\n",
        "#     location=LOCATION\n",
        "# )\n",
        "\n",
        "\n",
        "# Define the display name of your existing index endpoint\n",
        "DEPLOYED_INDEX_ID = \"mm_rag_langchain_index_endpoint\"\n",
        "\n",
        "# List all index endpoints with the specified display name\n",
        "index_endpoints = aiplatform.MatchingEngineIndexEndpoint.list(\n",
        "    filter=f'display_name=\"{DEPLOYED_INDEX_ID}\"',\n",
        "    location=LOCATION\n",
        ")\n",
        "\n",
        "# Check if any endpoints are found\n",
        "if index_endpoints:\n",
        "    # Use the first matching endpoint\n",
        "    index_endpoint = index_endpoints[0]\n",
        "    print(f\"Using existing index endpoint: {index_endpoint.resource_name}\")\n",
        "else:\n",
        "    print(\"No index endpoint found with the specified display name.\")\n",
        "    # Optionally, handle the case where no endpoint is found\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Replace with your existing Index's display name\n",
        "INDEX_DISPLAY_NAME = \"mm_rag_langchain_index\"\n",
        "\n",
        "# Replace with your existing Index Endpoint's display name\n",
        "INDEX_ENDPOINT_DISPLAY_NAME = \"mm_rag_langchain_index_endpoint\"\n",
        "\n",
        "# Function to retrieve an existing index by display name\n",
        "def get_existing_index(display_name):\n",
        "    indexes = aiplatform.MatchingEngineIndex.list()\n",
        "    for idx in indexes:\n",
        "        if idx.display_name == display_name:\n",
        "            return idx\n",
        "    raise ValueError(f\"No index found with display name '{display_name}'\")\n",
        "\n",
        "# Function to retrieve an existing index endpoint by display name\n",
        "def get_existing_index_endpoint(display_name):\n",
        "    index_endpoints = aiplatform.MatchingEngineIndexEndpoint.list()\n",
        "    for endpoint in index_endpoints[1:]:\n",
        "        if endpoint.display_name == display_name:\n",
        "            return endpoint\n",
        "    raise ValueError(f\"No index endpoint found with display name '{display_name}'\")\n",
        "\n",
        "# Retrieve the existing index\n",
        "index = get_existing_index(INDEX_DISPLAY_NAME)\n",
        "print(f\"Using existing index: {index.resource_name}\")\n",
        "\n",
        "# Retrieve the existing index endpoint\n",
        "index_endpoint = get_existing_index_endpoint(INDEX_ENDPOINT_DISPLAY_NAME)\n",
        "print(f\"Using existing index endpoint: {index_endpoint.resource_name}\")\n",
        "\n",
        "# Now you can use 'index' and 'index_endpoint' in your application\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on9oQe7_zX_J",
        "outputId": "5e6ae5e9-73b6-4a72-a10a-d02dd5c166bb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using existing index: projects/61515807285/locations/us-central1/indexes/8034399744996409344\n",
            "Using existing index endpoint: projects/61515807285/locations/us-central1/indexEndpoints/7796131177210839040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6adda75cab6"
      },
      "source": [
        "- Deploy Index to Index Endpoint\n",
        "  - NOTE: This will take a while to run.\n",
        "  - You can stop this cell after starting it instead of waiting for deployment.\n",
        "  - You can check the status at https://console.cloud.google.com/vertex-ai/matching-engine/indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4a02468a018b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af5b937-86bf-41a9-c421-890524573804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/61515807285/locations/us-central1/indexEndpoints/7796131177210839040\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/61515807285/locations/us-central1/indexEndpoints/7796131177210839040/operations/4307233069762347008\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/61515807285/locations/us-central1/indexEndpoints/7796131177210839040\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[id: \"multimodal_lucid_1730481025996\"\n",
              "index: \"projects/61515807285/locations/us-central1/indexes/7765309667261022208\"\n",
              "display_name: \"multimodal_lucid\"\n",
              "create_time {\n",
              "  seconds: 1730481050\n",
              "  nanos: 746459000\n",
              "}\n",
              "index_sync_time {\n",
              "  seconds: 1730563514\n",
              "  nanos: 504909000\n",
              "}\n",
              "automatic_resources {\n",
              "  min_replica_count: 2\n",
              "  max_replica_count: 2\n",
              "}\n",
              "deployment_group: \"default\"\n",
              ", id: \"mm_rag_langchain_index_endpoint1\"\n",
              "index: \"projects/61515807285/locations/us-central1/indexes/5289455772114092032\"\n",
              "create_time {\n",
              "  seconds: 1730560888\n",
              "  nanos: 776293000\n",
              "}\n",
              "index_sync_time {\n",
              "  seconds: 1730563565\n",
              "  nanos: 244132000\n",
              "}\n",
              "automatic_resources {\n",
              "  min_replica_count: 2\n",
              "  max_replica_count: 2\n",
              "}\n",
              "deployment_group: \"default\"\n",
              ", id: \"mm_rag_langchain_index_endpoint2\"\n",
              "index: \"projects/61515807285/locations/us-central1/indexes/8034399744996409344\"\n",
              "create_time {\n",
              "  seconds: 1730563479\n",
              "  nanos: 723943000\n",
              "}\n",
              "index_sync_time {\n",
              "  seconds: 1730563724\n",
              "  nanos: 378237000\n",
              "}\n",
              "automatic_resources {\n",
              "  min_replica_count: 2\n",
              "  max_replica_count: 2\n",
              "}\n",
              "deployment_group: \"default\"\n",
              "]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "index_endpoint = index_endpoint.deploy_index(\n",
        "    index=index, deployed_index_id=\"mm_rag_langchain_index_endpoint2\"\n",
        ")\n",
        "index_endpoint.deployed_indexes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd8475f61ef9"
      },
      "source": [
        "## Create retriever & load documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "487ad4e4ccac"
      },
      "source": [
        "- Create [`VectorSearchVectorStore`](https://api.python.langchain.com/en/latest/vectorstores/langchain_google_vertexai.vectorstores.vectorstores.VectorSearchVectorStore.html) with Vector Search Index ID and Endpoint ID.\n",
        "- Use [`textembedding-gecko`](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/text-embeddings) as embedding model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(index.name)\n",
        "print(index_endpoint.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfwdJK3zsKof",
        "outputId": "7e2d2dc1-375e-47db-aefe-fdca502cb807"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7796131177210839040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "e49355d04889"
      },
      "outputs": [],
      "source": [
        "# The vectorstore to use to index the summaries\n",
        "\n",
        "vectorstore = VectorSearchVectorStore.from_components(\n",
        "    project_id=PROJECT_ID,\n",
        "    region=LOCATION,\n",
        "    gcs_bucket_name=GCS_BUCKET,\n",
        "    index_id=index.name,\n",
        "    endpoint_id=index_endpoint.name,\n",
        "    embedding=VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME),\n",
        "    stream_update=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26ef209ff8ba"
      },
      "source": [
        "- Alternatively, use Chroma for a local vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1b7f713b2607",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "d6236450-8358-4de6-8b15-b87e83080838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic==1.10.12 in /usr/local/lib/python3.10/dist-packages (1.10.12)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.12) (4.12.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'model_validator' from 'pydantic' (/usr/local/lib/python3.10/dist-packages/pydantic/__init__.cpython-310-x86_64-linux-gnu.so)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-413269b7fb5d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pydantic==1.10.12'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVertexAIEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTYPE_CHECKING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_importer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectorStore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectorStoreRetriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_memory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInMemoryVectorStore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m __all__ = [\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"VectorStore\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_validator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'model_validator' from 'pydantic' (/usr/local/lib/python3.10/dist-packages/pydantic/__init__.cpython-310-x86_64-linux-gnu.so)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# # !pip install pydantic==1.10.12\n",
        "\n",
        "# from langchain.vectorstores import Chroma\n",
        "# from langchain.embeddings import VertexAIEmbeddings\n",
        "\n",
        "# vectorstore = Chroma(\n",
        "#     collection_name=\"mm_rag_test\",\n",
        "#     embedding_function=VertexAIEmbeddings(model_name=EMBEDDING_MODEL_NAME),\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67a4b0490b45"
      },
      "source": [
        "- Create Multi-Vector Retriever using the vector store you created.\n",
        "- Since vector stores only contain the embedding and an ID, you'll also need to create a document store indexed by ID to get the original source documents after searching for embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8e92ff890483"
      },
      "outputs": [],
      "source": [
        "docstore = InMemoryStore()\n",
        "\n",
        "id_key = \"doc_id\"\n",
        "# Create the multi-vector retriever\n",
        "retriever_multi_vector_img = MultiVectorRetriever(\n",
        "    vectorstore=vectorstore,\n",
        "    docstore=docstore,\n",
        "    id_key=id_key,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96b37cf7dc47"
      },
      "source": [
        "- Load data into Document Store and Vector Store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0a92a4b04319",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faee920a-eb72-472b-bd4c-260533c52c27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:Upserting datapoints MatchingEngineIndex index: projects/61515807285/locations/us-central1/indexes/8034399744996409344\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:MatchingEngineIndex index Upserted datapoints. Resource name: projects/61515807285/locations/us-central1/indexes/8034399744996409344\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['57fd1851-cb34-4713-82f7-362117e29a60',\n",
              " '2909c300-faa4-4db1-ac05-b5202077ebac',\n",
              " 'c77a4064-be2d-495d-a1fa-dea26d5aee5b',\n",
              " '200d9443-20e5-403a-a671-88996a3f6a8f']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Raw Document Contents\n",
        "doc_contents = texts + tables + img_base64_list\n",
        "\n",
        "doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
        "summary_docs = [\n",
        "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
        "    for i, s in enumerate(text_summaries + table_summaries + image_summaries)\n",
        "]\n",
        "\n",
        "retriever_multi_vector_img.docstore.mset(list(zip(doc_ids, doc_contents)))\n",
        "\n",
        "# If using Vertex AI Vector Search, this will take a while to complete.\n",
        "# You can cancel this cell and continue later.\n",
        "retriever_multi_vector_img.vectorstore.add_documents(summary_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b665ead18f3b"
      },
      "source": [
        "## Create Chain with Retriever and Gemini LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5228d5831d34"
      },
      "outputs": [],
      "source": [
        "def looks_like_base64(sb):\n",
        "    \"\"\"Check if the string looks like base64\"\"\"\n",
        "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
        "\n",
        "\n",
        "def is_image_data(b64data):\n",
        "    \"\"\"\n",
        "    Check if the base64 data is an image by looking at the start of the data\n",
        "    \"\"\"\n",
        "    image_signatures = {\n",
        "        b\"\\xFF\\xD8\\xFF\": \"jpg\",\n",
        "        b\"\\x89\\x50\\x4E\\x47\\x0D\\x0A\\x1A\\x0A\": \"png\",\n",
        "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
        "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
        "    }\n",
        "    try:\n",
        "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
        "        for sig, format in image_signatures.items():\n",
        "            if header.startswith(sig):\n",
        "                return True\n",
        "        return False\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def split_image_text_types(docs):\n",
        "    \"\"\"\n",
        "    Split base64-encoded images and texts\n",
        "    \"\"\"\n",
        "    b64_images = []\n",
        "    texts = []\n",
        "    for doc in docs:\n",
        "        # Check if the document is of type Document and extract page_content if so\n",
        "        if isinstance(doc, Document):\n",
        "            doc = doc.page_content\n",
        "        if looks_like_base64(doc) and is_image_data(doc):\n",
        "            b64_images.append(doc)\n",
        "        else:\n",
        "            texts.append(doc)\n",
        "    return {\"images\": b64_images, \"texts\": texts}\n",
        "\n",
        "\n",
        "def img_prompt_func(data_dict):\n",
        "    \"\"\"\n",
        "    Join the context into a single string\n",
        "    \"\"\"\n",
        "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
        "    messages = [\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": (\n",
        "                \"You are financial analyst tasking with providing investment advice.\\n\"\n",
        "                \"You will be given a mix of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
        "                \"Use this information to provide investment advice related to the user's question. \\n\"\n",
        "                f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
        "                \"Text and / or tables:\\n\"\n",
        "                f\"{formatted_texts}\"\n",
        "            ),\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Adding image(s) to the messages if present\n",
        "    if data_dict[\"context\"][\"images\"]:\n",
        "        for image in data_dict[\"context\"][\"images\"]:\n",
        "            messages.append(\n",
        "                {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
        "                }\n",
        "            )\n",
        "    return [HumanMessage(content=messages)]\n",
        "\n",
        "\n",
        "# Create RAG chain\n",
        "chain_multimodal_rag = (\n",
        "    {\n",
        "        \"context\": retriever_multi_vector_img | RunnableLambda(split_image_text_types),\n",
        "        \"question\": RunnablePassthrough(),\n",
        "    }\n",
        "    | RunnableLambda(img_prompt_func)\n",
        "    | ChatVertexAI(\n",
        "        temperature=0,\n",
        "        model_name=MODEL_NAME,\n",
        "        max_output_tokens=TOKEN_LIMIT,\n",
        "    )  # Multi-modal LLM\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2925d397fdbb"
      },
      "source": [
        "## Process user query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "b3b445f934a8"
      },
      "outputs": [],
      "source": [
        "query = \"\"\"\n",
        " - What is the purpose of the GitLab Handbook?\n",
        " - Can you explain why GitLab prefers documenting information in the handbook?\n",
        " - When did GitLab start the handbook and why?\n",
        " - How has the size of the GitLab Handbook changed over the years?\n",
        " - What are some benefits of using a handbook according to GitLab?\n",
        " - How does GitLab believe the handbook aids in team onboarding?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6172a22b1203"
      },
      "source": [
        "### Get Retrieved documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "90a34d3712e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9dcaa9c-e923-41cd-9618-5e3dc048f970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-2dbe6f3f2a6e>:2: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever_multi_vector_img.get_relevant_documents(query, limit=10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['22/10/2024, 14:54\\n\\nAbout the Handbook | The GitLab Handbook\\n\\nThe GitLab Handbook\\n\\nGitLab TeamOps Handbook Job Families Reports\\n\\nAbout the Handbook\\n\\nHistory of the handbook\\n\\nThe handbook started when GitLab was a company of just ten people to make sharing information efficient and easy. We knew that future GitLab team-members wouldnʼt be able to see emails about process changes that were being sent before they joined and that most of the people who would eventually join GitLab likely hadnʼt even heard of us yet. The handbook was our way of ensuring that all of our company information was accessible to everyone regardless of when they became part of the team.\\n\\nAdvantages\\n\\nAt GitLab our handbook is extensive and keeping it relevant is an important part of\\n\\neveryoneʼs job. It is a vital part of who we are and how we communicate. We established these processes because we saw these benefits:\\n\\n. Reading is much faster than listening.\\n\\n. Reading is async, you donʼt have to interrupt someone or wait for them to become\\n\\navailable.\\n\\n. Talent Acquisition is easier if people can see what we stand for and how we operate.\\n\\n. Retention is better if people know what they are getting into before they join. . On-boarding is easier if you can find all relevant information spelled out. . Teamwork is easier if you can read how other parts of the company work. . Discussing changes is easier if you can read what the current process is. . Communicating change is easier if you can just point to the diff.\\n\\n. Everyone can contribute to it by proposing a change via a merge request.\\n\\nOo . Everyone can contribute to it by proposing a change via a merge request.\\n\\nOne common concern newcomers to the handbook express is that the strict\\n\\ndocumentation makes the company more rigid. In fact, writing down our current process in the handbook has the effect of empowering contributors to propose change. As a result, this handbook is far from rigid. You only need to look at the handbook changelog to see the evidence. Every attempt is made to document guidelines and processes in the handbook. However, it is not possible to document every possible situation or scenario that could potentially occur. Just because something is not yet in the handbook does not mean that it\\n\\nhttps://handbook.gitlab.com/handbook/about/\\n\\n1/6\\n\\n22/10/2024, 14:54\\n\\nAbout the Handbook | The GitLab Handbook\\n\\nis allowed. GitLab will review each team memberʼs concern or situation based on local laws to determine the best outcome and then update the handbook accordingly. If you have questions, please discuss with your manager or contact the People Success team.', 'https://handbook.gitlab.com/handbook/about/\\n\\n2/6\\n\\n22/10/2024, 14:54\\n\\nAbout the Handbook | The GitLab Handbook\\n\\nDate\\n\\nDate\\n\\nWord Count Page Count\\n\\nNotes', 'Handbook Interpretation\\n\\nThe handbook is subject to interpretation. We do our best to be as clear as possible to minimize confusion and/or misinterpretation. We also recognize that we have a global audience and that may bring different interpretations. If you have any questions or need further clarification please check with the content owner of the page. When in doubt please reach out and ask.\\n\\nRemember that everything is in draft at GitLab and subject to change, this includes our handbook.\\n\\nCount handbook pages\\n\\nItʼs easy to see that the handbook is large, but have you ever wondered just how large? The handbook is over two thousand pages long. Thatʼs a lot of good info!\\n\\nHistorical Word and Page Counts\\n\\nabout.gitlab.com/handbook\\n\\nDate\\n\\nWord Count Page Count\\n\\nNotes', 'handbook.gitlab.com/handbook\\n\\nDate\\n\\nWord Count Page Count\\n\\nNotes']\n"
          ]
        }
      ],
      "source": [
        "# List of source documents\n",
        "docs = retriever_multi_vector_img.get_relevant_documents(query, limit=10)\n",
        "\n",
        "source_docs = split_image_text_types(docs)\n",
        "\n",
        "print(source_docs[\"texts\"])\n",
        "\n",
        "for i in source_docs[\"images\"]:\n",
        "    display(Image(base64.b64decode(i)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd784ce7f205"
      },
      "source": [
        "### Get generative response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4c5f936f89da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "2c129415-ff2f-41ad-89fe-bbd9c45107cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "## GitLab Handbook: A Deep Dive into its Purpose and Benefits\n\nThe GitLab Handbook is a comprehensive document that serves as the central repository of information for the company. It was created in the early days of GitLab, when the company was only ten people strong, to ensure efficient and accessible information sharing. \n\nHere's a breakdown of your questions and their answers:\n\n**1. Purpose of the GitLab Handbook:**\n\nThe handbook's primary purpose is to provide a single source of truth for all company information, processes, and guidelines. It aims to:\n\n* **Make information accessible to everyone:** Regardless of when they joined the company.\n* **Ensure consistency and clarity:** By documenting processes and procedures.\n* **Facilitate onboarding:** By providing new hires with a comprehensive overview of the company's culture and operations.\n* **Promote transparency and collaboration:** By allowing everyone to contribute to the handbook through merge requests.\n\n**2. Why GitLab prefers documenting information in the handbook:**\n\nGitLab believes that documenting information in a handbook offers several advantages over other methods:\n\n* **Efficiency:** Reading is faster than listening, allowing for quicker information absorption.\n* **Asynchronous access:** Information can be accessed anytime, anywhere, without interrupting others.\n* **Improved talent acquisition:** Potential candidates can understand GitLab's values and operations before joining.\n* **Enhanced retention:** Employees are better informed about the company's culture and expectations.\n* **Simplified onboarding:** New hires can easily find relevant information.\n* **Streamlined teamwork:** Understanding how different parts of the company operate fosters collaboration.\n* **Easier change management:** Discussing and communicating changes is simplified by referencing the handbook.\n\n**3. When did GitLab start the handbook and why?**\n\nThe GitLab Handbook was started when the company was only ten people. The founders recognized the need for a centralized source of information to ensure that everyone, regardless of their start date, had access to the same information. This was particularly important as GitLab was growing rapidly and new employees needed to be quickly integrated into the company culture.\n\n**4. How has the size of the GitLab Handbook changed over the years?**\n\nThe handbook has grown significantly over the years, reflecting the growth of GitLab itself. It is currently over two thousand pages long, showcasing the vast amount of information it contains. The provided tables show the historical word and page counts, demonstrating the continuous expansion of the handbook.\n\n**5. Benefits of using a handbook according to GitLab:**\n\nGitLab highlights several benefits of using a handbook:\n\n* **Transparency:** Everyone has access to the same information, fostering a culture of openness.\n* **Consistency:** Standardized processes and guidelines ensure consistency across the company.\n* **Empowerment:** Documenting processes allows for easier identification of areas for improvement and encourages contributions from all team members.\n* **Flexibility:** While the handbook provides a framework, it is not rigid and is constantly evolving to reflect changing needs.\n\n**6. How does GitLab believe the handbook aids in team onboarding:**\n\nThe handbook serves as a comprehensive onboarding resource for new hires. It provides information on:\n\n* **Company culture and values:** Helping new employees understand the company's philosophy and expectations.\n* **Processes and procedures:** Providing a clear understanding of how things work at GitLab.\n* **Team structure and roles:** Giving new hires context for their role within the organization.\n* **Tools and resources:** Providing access to the necessary tools and resources for success.\n\nBy providing this comprehensive information, the handbook helps new hires quickly integrate into the company and become productive members of the team.\n\n**Investment Advice:**\n\nWhile the GitLab Handbook itself is not an investment opportunity, understanding its purpose and benefits can provide valuable insights into the company's culture and operations. This information can be helpful for investors looking to assess GitLab's long-term growth potential and its ability to attract and retain talent. \n"
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "result = chain_multimodal_rag.invoke(query)\n",
        "\n",
        "Markdown(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwNrHCqbi3xi"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05jynhZnkgxn"
      },
      "source": [
        "Congratulations on making it through this multimodal RAG notebook!\n",
        "\n",
        "While multimodal RAG can be quite powerful, note that it can face some limitations:\n",
        "\n",
        "* **Data dependency:** Needs high-accuracy data from the text and visuals.\n",
        "* **Computationally demanding:** Generating embeddings from multimodal data is resource-intensive.\n",
        "* **Domain specific:** Models trained on general data may not shine in specialized fields like medicine.\n",
        "* **Black box:** Understanding how these models work can be tricky, hindering trust and adoption.\n",
        "\n",
        "\n",
        "Despite these challenges, multimodal RAG represents a significant step towards search and retrieval systems that can handle diverse, multimodal data."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "multimodal_rag_langchain.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}